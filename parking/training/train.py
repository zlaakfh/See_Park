import argparse
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, WeightedRandomSampler
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from torch.amp import GradScaler
from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau

# ë¶„ë¦¬í•œ ëª¨ë“ˆë“¤ ì„í¬íŠ¸
import config as cfg
from dataset import MultiCamDrivingDataset
import trainer
from mobilenetv3s_parking_model_pretrained_multi import MultiCamParkingModel # ê¸°ì¡´ ëª¨ë¸ íŒŒì¼

"""
python train.py \
--csv_path "/home/elicer/data/valet_parking_final/aug_final.csv" \
--img_root "/home/elicer/data/valet_parking_final" \
--out_dir_prefix "mobilenetv3s_pretrained_up_mid_crop_LR_cls_05_sampler_final" \
--epochs 100 \
--scheduler "onecycle"

python train.py \
--csv_path "/home/elicer/data/valet_parking_final/aug_final_with04.csv" \
--img_root "/home/elicer/data/valet_parking_final" \
--out_dir_prefix "mobilenetv3s_pretrained_up_mid_crop_LR_cls_05_sampler_final_with04" \
--epochs 100 \
--scheduler "onecycle"

python train.py \
--csv_path "/home/elicer/data/valet_parking_final/aug_final.csv" \
--img_root "/home/elicer/data/valet_parking_final" \
--out_dir_prefix "mobilenetv3s_pretrained_up_mid_crop_LR_cls_05_sampler_final" \
--epochs 200 \
--scheduler "onecycle"

python train.py \
--csv_path "/home/elicer/data/valet_parking_final/aug_final_with04.csv" \
--img_root "/home/elicer/data/valet_parking_final" \
--out_dir_prefix "mobilenetv3s_pretrained_up_mid_crop_LR_cls_05_sampler_final_with04" \
--epochs 200 \
--scheduler "onecycle"
"""

NUM_WORKERS = 4

def create_sampler(df, class_col_name):
    y_train = df[class_col_name].values.astype(int)

    print(f"   Sampler Check -> Column: {class_col_name}")
    print(f"   Unique Values: {np.unique(y_train)}")

    class_counts = np.bincount(y_train)
    print(f"   Class Counts in Train: {class_counts}")
    
    class_weights = 1. / class_counts
    sample_weights = class_weights[y_train]
    sample_weights = torch.from_numpy(sample_weights).double()
    
    sampler = WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights),
        replacement=True
    )
    return sampler

def main():
    # 1. Arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('--csv_path', type=str, required=True)
    parser.add_argument('--img_root', type=str, required=True)
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch_size', type=int, default=256) 
    parser.add_argument('--lr', type=float, default=0.001)
    parser.add_argument('--scheduler', type=str, default='onecycle', choices=['onecycle', 'cosine', 'cosine_restart', 'plateau'])
    parser.add_argument('--patience', type=int, default=20)
    parser.add_argument('--out_dir_prefix', type=str, required=True)
    
    args = parser.parse_args()

    # ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
    OUT_BASE = "/home/elicer/hyun_ws/E2E/parking/trained/multi/model"
    lr_str = f"{args.lr:.3f}".replace('.', '')
    dir_name = f"{args.out_dir_prefix}_{args.scheduler}_batch{args.batch_size}_epoch{args.epochs}_lr{lr_str}"
    args.out_dir = os.path.join(OUT_BASE, dir_name)
    os.makedirs(args.out_dir, exist_ok=True)
    
    print("ğŸ“ Output dir:", args.out_dir)
    
    # 2. Setup
    # A100 TensorCore í™œìš© ì„¤ì •
    torch.set_float32_matmul_precision('high')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ Device: {device} (A100 Optimization Enabled)")

    # 3. Data Preparation
    print(f"ğŸ“‚ Reading CSV: {args.csv_path}")
    df = pd.read_csv(args.csv_path)
    
    # ================= [í•„ìˆ˜ ì¶”ê°€ ì½”ë“œ] ë¹ˆ ê°’ ì²­ì†Œ =================
    # configì— ì„¤ì •ëœ í´ë˜ìŠ¤ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
    target_col = cfg.CLASS_COL_NAME  # 'sign_class'
    
    # 1. ë¹ˆ ê°’(NaN)ì´ ìˆëŠ”ì§€ í™•ì¸
    nan_count = df[target_col].isnull().sum()
    if nan_count > 0:
        print(f"âš ï¸ Warning: Found {nan_count} rows with NaN (empty) in '{target_col}'. Dropping them!")
        # ë¹ˆ ê°’ì´ ìˆëŠ” í–‰ì„ ì•„ì˜ˆ ì‚­ì œí•´ë²„ë¦¼
        df = df.dropna(subset=[target_col])
        
    # 2. ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ì¤‘ê°„ì— ë¹ ì§„ í–‰ ì •ë¦¬)
    df = df.reset_index(drop=True)

    # 3. ì´ì œ ì•ˆì „í•˜ê²Œ intë¡œ ë³€í™˜
    df[target_col] = df[target_col].astype(int)
    print(f"âœ… Data Cleaned. Valid samples: {len(df)}")
    # =============================================================

    temp_df, test_df = train_test_split(df, test_size=0.1, random_state=42)
    train_df, val_df = train_test_split(temp_df, test_size=0.2, random_state=42)
    print(f"ğŸ“Š Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")

    # Sampler ìƒì„±
    sampler = create_sampler(train_df, cfg.CLASS_COL_NAME)

    # DataLoader
    train_ds = MultiCamDrivingDataset(train_df, args.img_root)
    val_ds   = MultiCamDrivingDataset(val_df, args.img_root)
    test_ds  = MultiCamDrivingDataset(test_df, args.img_root)                                                   

    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=False, sampler=sampler, num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)
    val_loader   = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)
    test_loader  = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)

    trainer.check_sampler_balance(train_loader)

    # 4. Model & Optimizer
    # PRE ì„¤ì •ì„ ì¸ìë¡œ ë°›ê±°ë‚˜ ì—¬ê¸°ì„œ ê³ ì •
    PRE = True 
    model = MultiCamParkingModel(pretrained=PRE, num_classes=cfg.NUM_CLASSES).to(device)
    
    criterion_reg = nn.MSELoss()
    criterion_cls = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)
    scaler = GradScaler("cuda")

    # Scheduler setup
    scheduler = None
    if args.scheduler == 'onecycle':
        scheduler = OneCycleLR(optimizer, max_lr=args.lr, steps_per_epoch=len(train_loader), epochs=args.epochs)
    elif args.scheduler == 'cosine':
        scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs)
    elif args.scheduler == 'cosine_restart':
        scheduler = CosineAnnealingWarmRestarts(
            optimizer,
            T_0=20,        # ì²« ì£¼ê¸° (epoch)
            T_mult=2,      # ì£¼ê¸° ì¦ê°€ ë°°ìˆ˜
            eta_min=1e-6   # min_lr
        )
    elif args.scheduler == 'plateau':
        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6)

    # 5. ==================================================== Training Loop =======================================================
    os.makedirs(args.out_dir, exist_ok=True)
    writer = SummaryWriter(log_dir=os.path.join(args.out_dir, "logs"))
    best_val_loss = float('inf')
    early_stop_cnt = 0

    print(f"\nğŸ”¥ Start Training...")
    for epoch in range(args.epochs):
        is_onecycle = (args.scheduler == 'onecycle')
        
        # trainer ëª¨ë“ˆì˜ í•¨ìˆ˜ ì‚¬ìš©
        train_loss, train_reg, train_cls = trainer.train_one_epoch(
            model, train_loader, criterion_reg, criterion_cls, optimizer, device, scaler, scheduler, is_onecycle
        )
        
        # [ìˆ˜ì •ë¨] ë°˜í™˜ê°’ ê°œìˆ˜ì™€ ìˆœì„œê°€ trainer.pyì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
        (val_loss, val_reg, val_cls,         # Loss 3ê°œ
         val_lin_mae, val_ang_mae,           # MAE 2ê°œ
         val_acc, val_class_accs,            # Acc ê´€ë ¨
         val_prec, val_recall, val_f1, val_cm, # Cls Metrics
         all_val_targets_lin, all_val_preds_lin, # Scatterìš© ë°ì´í„°
         all_val_targets_ang, all_val_preds_ang) = trainer.eval_one_epoch(
            model, val_loader, criterion_reg, criterion_cls, device
        )

        if args.scheduler == 'cosine': scheduler.step()
        elif args.scheduler == 'plateau': scheduler.step(val_loss)

        current_lr = trainer.get_lr(optimizer)

        # ---------------- Tensorboard Logging ----------------
        # 1. [Total Loss]
        writer.add_scalars("Total_Loss", {"Train": train_loss, "Val": val_loss}, epoch)

        # 2. [Task Loss]
        # train.py ìˆ˜ì • ì œì•ˆ
        writer.add_scalars("Loss/Reg", {"Train": train_reg, "Val": val_reg}, epoch)
        writer.add_scalars("Loss/Cls", {"Train": train_cls, "Val": val_cls}, epoch)

        # 3. [Validation Metrics]
        writer.add_scalar("Val_Metrics/MAE_Linear", val_lin_mae, epoch)
        writer.add_scalar("Val_Metrics/MAE_Angular", val_ang_mae, epoch)
        writer.add_scalar("Val_Metrics/Accuracy", val_acc, epoch)
        writer.add_scalar("Val_Metrics/Precision", val_prec, epoch)
        writer.add_scalar("Val_Metrics/Recall", val_recall, epoch)
        writer.add_scalar("Val_Metrics/F1_Score", val_f1, epoch)

        # 4. [Class Accuracy]
        writer.add_scalar("Val_Class_Acc/0_Drive", val_class_accs[0], epoch)
        writer.add_scalar("Val_Class_Acc/1_Parked", val_class_accs[1], epoch)

        # 5. [LR]
        writer.add_scalar("LR", current_lr, epoch)

        # 6. [Confusion Matrix]
        class_names = ["Drive", "Parked"] 
        cm_figure = trainer.plot_confusion_matrix(val_cm, class_names)
        writer.add_figure("Confusion Matrix", cm_figure, epoch)
        plt.close(cm_figure)

        # 7. [Linear, Angular ê°’ ì‹œê°í™”]
        fig_lin = trainer.plot_regression_scatter(all_val_targets_lin, all_val_preds_lin, "Linear Output Analysis")
        writer.add_figure("Analysis/Linear_Scatter", fig_lin, epoch)
        plt.close(fig_lin)

        fig_ang = trainer.plot_regression_scatter(all_val_targets_ang, all_val_preds_ang, "Angular Output Analysis")
        writer.add_figure("Analysis/Angular_Scatter", fig_ang, epoch)
        plt.close(fig_ang)

        # 8. [Histogram]
        # ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ ë¶„í¬ê°€ ì •ë‹µ ë¶„í¬ì™€ ë¹„ìŠ·í•œ ëª¨ì–‘ì¸ì§€ í™•ì¸
        writer.add_histogram("Dist/Linear_Pred", np.array(all_val_preds_lin), epoch)
        writer.add_histogram("Dist/Linear_GT",   np.array(all_val_targets_lin), epoch)

        # ---------------- Terminal Output ----------------
        print(f"Epoch [{epoch+1}/{args.epochs}] | LR: {current_lr:.8f} | Total Val Loss: {val_loss:.5f}")
        print(f"   >> [Reg] MAE Lin: {val_lin_mae:.4f}, Ang: {val_ang_mae:.4f}")
        print(f"   >> [Cls] Acc: {val_acc:.2f}%")
        print(f"   >> [Detail] Drive: {val_class_accs[0]:.2f}% | Parked: {val_class_accs[1]:.2f}%")
        print(f"   >> [Metrics] Pre: {val_prec:.4f} | Rec: {val_recall:.4f} | F1: {val_f1:.4f}")
        print(f"   >> Confusion Matrix:\n{val_cm}")
   
        # Save Best
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), os.path.join(args.out_dir, "best_model.pth"))
            early_stop_cnt = 0
            print("âœ… Best Model Saved!")
        else:
            early_stop_cnt += 1
            if early_stop_cnt >= args.patience:
                print(f"ğŸ›‘ Early Stopping at epoch {epoch+1}")
                break
    
    writer.close()
    # =================================================================================================================================================================================

    # ---------------------------------------------------------
    # ìµœì¢… í…ŒìŠ¤íŠ¸ ë‹¨ê³„ (Test Loop)
    # ---------------------------------------------------------
    print("\nğŸ” Starting Final Test with the Best Model...")
    
    # 1. ì €ì¥ëœ ìµœê³ ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°
    # ê·¸ëƒ¥ modelì„ ì“°ë©´ 'ë§ˆì§€ë§‰ ì—í¬í¬'ì˜ ê°€ì¤‘ì¹˜ë¼ì„œ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€ ìƒíƒœê°€ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    best_model_path = os.path.join(args.out_dir, "best_model.pth")
    
    if os.path.exists(best_model_path):
        checkpoint = torch.load(best_model_path, map_location=device)
        model.load_state_dict(checkpoint)
        print(f"âœ… Loaded weights from {best_model_path}")
    else:
        print("âš ï¸ Warning: Best model not found. Testing with final epoch weights.")

    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€ ì‹¤í–‰
    # eval_one_epoch í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì¬í™œìš©í•˜ë©´ ë©ë‹ˆë‹¤.
    (test_loss, test_reg, test_cls, 
     test_lin_mae, test_ang_mae, 
     test_acc, test_class_accs, 
     test_prec, test_recall, test_f1, test_cm,
     _, _, _, _) = trainer.eval_one_epoch(model, test_loader, criterion_reg, criterion_cls, device)
    
    print("="*40)
    print(f"ğŸ† Final Test Result")
    print(f"Loss: {test_loss:.6f}")
    print(f"Reg MAE -> Linear: {test_lin_mae:.4f}, Angular: {test_ang_mae:.4f}")
    print(f"Acc -> {test_acc:.2f}%")
    print(f"   >> [Detail] Drive: {test_class_accs[0]:.2f}% | Parked: {test_class_accs[1]:.2f}%")
    print(f"   >> [Metrics] Pre: {test_prec:.4f} | Rec: {test_recall:.4f} | F1: {test_f1:.4f}")
    print(f"   >> Confusion Matrix:\n{test_cm}")
    print("="*40)
    
    print("ğŸ Training & Testing Finished.")

if __name__ == "__main__":
    main()